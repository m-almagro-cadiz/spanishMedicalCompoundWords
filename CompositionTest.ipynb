{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, tables, pickle\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathCieSnFrequencies = 'data/cie_sn_frequencies.csv'\n",
    "pathTags = 'data/tags.csv'\n",
    "pathLemmas = 'data/extendedLemmas.csv'\n",
    "pathPrefixes = 'data/medicalPrefixes.csv'\n",
    "pathOtherPrefixes = 'data/otherMedicalPrefixes.csv'\n",
    "pathSuffixes = 'data/medicalSuffixes.csv'\n",
    "pathWordIndex = 'data/wordIndex.obj'\n",
    "pathConceptNetEmbeddings = 'embeddings/'\n",
    "\n",
    "with open(pathCieSnFrequencies, encoding='utf8') as f: freq = Counter({line.strip().split('\t')[0]:int(line.strip().split('\t')[1]) for line in re.split('\\n', f.read()) if line})\n",
    "\n",
    "with open(pathTags, 'r', encoding='utf8') as f: wordTag = {line.strip().split('\\t')[0]:{element.split(' ')[0]:element.split(' ')[1] for element in line.strip().split('\\t')[1::]} for line in re.split('\\n', f.read())}\n",
    "\n",
    "with open(pathLemmas, 'r', encoding='utf8') as f: lemmaList = [line.strip().split('\\t') for line in re.split('\\n', f.read())]\n",
    "\n",
    "with open(pathPrefixes, encoding='utf8') as f: medicalPrefixes = {prefix.split('\\t')[0].lower().replace('-', '').strip():prefix.split('\\t')[1].strip() if len(prefix.split('\\t')) > 1 else '' for prefix in re.split('\\n', f.read()) if prefix}\n",
    "\n",
    "with open(pathOtherPrefixes, encoding='utf8') as f: otherMedicalPrefixes = {prefix.split('\\t')[0]:prefix.split('\\t')[1] if len(prefix.split('\\t')) > 1 else '' for prefix in re.split('\\n', f.read()) if prefix}\n",
    "\n",
    "with open(pathSuffixes, encoding='utf8') as f: medicalSuffixes = {prefix.split('\\t')[0].lower().replace('-', '').strip():prefix.split('\\t')[1].strip() if len(prefix.split('\\t')) > 1 else '' for prefix in re.split('\\n', f.read()) if prefix}\n",
    "\n",
    "with open(pathWordIndex, 'rb') as f: wordIndex = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAbsFreq(word):\n",
    "   lemmas_ = [word]\n",
    "   if word in splittedLemmas:\n",
    "      lemmas_ = splittedLemmas[word]\n",
    "   elif word not in freq:\n",
    "      word_ = unidecode(word).lower()\n",
    "      if word_ in splittedLemmas:\n",
    "         lemmas_ = splittedLemmas[word_]\n",
    "   f = 0\n",
    "   for lemma_ in lemmas_:\n",
    "      if lemma_ in freq:\n",
    "         f += freq[lemma_]\n",
    "   return f\n",
    "\n",
    "def getRelFreq(word):\n",
    "   return getAbsFreq(word) / freqAcc\n",
    "\n",
    "def getTags(word):\n",
    "   return tagger[word] if word in tagger else {}\n",
    "\n",
    "def isSharedElement(tags1, tags2):\n",
    "   shared = False\n",
    "   for tag in tags1:\n",
    "      if tag in tags2:\n",
    "         shared = True\n",
    "         break\n",
    "   return shared\n",
    "\n",
    "def getCandidateSubwords(word, prefixes, suffixes=[], reverse=False):\n",
    "   possibilities = list()\n",
    "   if len(word) > 3:\n",
    "      word_ = ''.join(reversed(word)) if reverse else word\n",
    "      i, subwords, subwords__ = 0, [[(list(), word_.lower())]], list()\n",
    "      while i < len(prefixes):\n",
    "         if len(subwords) == 1 or len(prefixes[i]) > 2:\n",
    "            for subword in subwords[-1]:\n",
    "               if subword[1].startswith(prefixes[i]):\n",
    "                  #Getting subword\n",
    "                  subword_ = re.sub(r'^\\W+', '', subword[1][len(prefixes[i])::]) #Deleting '-'\n",
    "                  if subword_ and prefixes[i][-1] in ['a', 'e', 'i', 'o', 'u']: #Deleting duplicated r\n",
    "                     subword_ = re.sub(r'^r(r)', r'\\1', subword_)\n",
    "                     if subword_ and len(prefixes[i]) > 1 and prefixes[i][-1] != subword_[0]: #Adding missing letter\n",
    "                        subwords__.append((subword[0] + [prefixes[i]], prefixes[i][-1] + subword_))\n",
    "                  startsWith_bp = prefixes[i][-1] != 'm' or (len(subword_) > 0 and subword_[0] in ['b', 'p'])\n",
    "                  notStartsWith_bp = prefixes[i][-1] != 'n' or (len(subword_) > 0 and subword_[0] not in ['b', 'p'])\n",
    "                  if (startsWith_bp and notStartsWith_bp) or reverse:\n",
    "                     subwords__.append((subword[0] + [prefixes[i]], subword_))\n",
    "         i += 1\n",
    "         if i == len(prefixes) and len(subwords__) > 0:\n",
    "            subwords.append(subwords__)\n",
    "            i, subwords__ = 0, list()\n",
    "      subwords = [([''.join(reversed(prefix)) for prefix in reversed(e[0])], ''.join(reversed(e[1]))) if reverse else (e[0], e[1]) for group in reversed(subwords[1::]) for e in group]\n",
    "      wtag, fWord = getTags(word), getRelFreq(word) #Getting whole word tag and frequency\n",
    "      for subword in subwords:\n",
    "         fSubword = getRelFreq(subword[1])\n",
    "         notShort_ = len(subword[1]) > 3\n",
    "         inSuffixes_ = subword[1] in suffixes\n",
    "         if (fSubword > 0 and notShort_):\n",
    "            swtag = getTags(subword[1]) #Getting subword tag\n",
    "            matchingTag = isSharedElement(wtag, swtag) #Comparing tags\n",
    "            matching = len(wtag) == 0 or len(swtag) == 0 or matchingTag\n",
    "            isVerb = ('v' in wtag and len(wtag) == 1) or ('v' in swtag and len(swtag) == 1)\n",
    "            em = -1\n",
    "            if word in wordIndex and subword[1] in wordIndex:\n",
    "               nb1 = pd.read_hdf(pathConceptNetEmbeddings + str(wordIndex[word][1]) + '.h5')\n",
    "               if str(wordIndex[word][1]) == str(wordIndex[subword[1]][1]):\n",
    "                  nb2 = pd.read_hdf(pathConceptNetEmbeddings + str(wordIndex[subword[1]][1]) + '.h5')\n",
    "               else:\n",
    "                  nb2 = pd.read_hdf(pathConceptNetEmbeddings + str(wordIndex[subword[1]][1]) + '.h5')\n",
    "               em = cosine_similarity([nb1.loc[wordIndex[word][0]].values, nb2.loc[wordIndex[subword[1]][0]].values])[0][1]\n",
    "            elif subword[1] in wordIndex:\n",
    "               em = -2\n",
    "            if fSubword == 0:\n",
    "               rate_ = -1\n",
    "            else:\n",
    "               rate_ = fWord  / fSubword\n",
    "            possibilities.append((subword, fWord, fSubword, rate_, em, 1 if matching else 0, 1 if isVerb else 0, len(subword[1]), len(subword[0]), len(''.join(subword[0])), 1 if inSuffixes_ else 0))\n",
    "      possibilities = sorted(possibilities, key=lambda tup: (-tup[7], tup[8], tup[3], -tup[4]))\n",
    "   return possibilities\n",
    "\n",
    "def isCandidate(candidate):\n",
    "   if (candidate[3] < 5) or (candidate[2] > 0.00001 and candidate[3] < 10):\n",
    "      return True\n",
    "   else:\n",
    "      return False\n",
    "\n",
    "def selectCandidate(candidates):\n",
    "   candidates_ = [candidate for candidate in candidates if isCandidate(candidate)]\n",
    "   if len(candidates_) > 0:\n",
    "      min_l_subword = min([candidate[7] for candidate in candidates_])\n",
    "      min_n_prefix = min([candidate[8] for candidate in candidates_ if candidate[7] == min_l_subword])\n",
    "      min_l_prefix = min([candidate[9] for candidate in candidates_ if candidate[8] == min_n_prefix])\n",
    "      min_conditions = [1 if candidate[7] == min_l_subword and candidate[8] == min_n_prefix and candidate[9] == min_l_prefix else 0 for candidate in candidates_]\n",
    "      conditions = [1 if min_conditions[i] == 1 and candidates_[i][6] == 0 and candidates_[i][2] > 0.0000005 and (candidates_[i][4] >= 0.4 or candidates_[i][4] <= -1) else 0 for i in range(len(candidates_))]\n",
    "      candidates_ = [candidates_[i] for i in range(len(candidates_)) if conditions[i] == 1]\n",
    "   return candidates_\n",
    "\n",
    "def divideCompoundWord(word):\n",
    "   candidates = getCandidateSubwords(word, prefixes, suffixes)\n",
    "   compositions = selectCandidate(candidates)\n",
    "   if len(compositions) > 0:\n",
    "      return [medicalPrefixes[prefix] for prefix in compositions[0][0][0]] + [compositions[0][0][1]]\n",
    "   else:\n",
    "      return [word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building lemmas dictionary\n",
    "lemmas = dict()\n",
    "for line in lemmaList:\n",
    "   key = line[0].split(' ')\n",
    "   if len(key) == 1:\n",
    "      key.append('')\n",
    "   key = tuple(key)\n",
    "   values = line[1::]\n",
    "   for v in range(len(values)):\n",
    "      value = values[v].split(' ')\n",
    "      if len(value) == 1:\n",
    "         value.append('')\n",
    "      value = tuple(value)\n",
    "      values[v] = value\n",
    "   if key not in lemmas:\n",
    "      lemmas[key] = list()\n",
    "   lemmas[key] = values\n",
    "\n",
    "#Extending tags from lemmas\n",
    "tagger = {word:{tag[0] for tag in wordTag[word]} for word in wordTag}\n",
    "for lemma in lemmas:\n",
    "   if lemma[0] not in tagger:\n",
    "      tagger[lemma[0]] = set()\n",
    "   tagger[lemma[0]].add(lemma[1])\n",
    "\n",
    "#Collecting derivational words without tags\n",
    "splittedLemmas = dict()\n",
    "for lemma in lemmas:\n",
    "   if lemma[0] not in splittedLemmas:\n",
    "      splittedLemmas[lemma[0]] = set()\n",
    "   splittedLemmas[lemma[0]].update([lemma_[0] for lemma_ in lemmas[lemma]])\n",
    "   word_ = unidecode(lemma[0]).lower()\n",
    "   if word_ not in lemmas:\n",
    "      if word_ not in splittedLemmas:\n",
    "         splittedLemmas[word_] = set()\n",
    "      splittedLemmas[word_].update([lemma_[0] for lemma_ in lemmas[lemma]])\n",
    "\n",
    "#Gathering word counts from CIE-10-ES and SNOMED-CT using lemmas\n",
    "freq_ = dict()\n",
    "for w in freq:\n",
    "   lemmas_ = [w]\n",
    "   if w in splittedLemmas:\n",
    "      lemmas_ = splittedLemmas[w]\n",
    "   f = freq[w]\n",
    "   for lemma_ in lemmas_:\n",
    "      if lemma_ not in freq_:\n",
    "         freq_[lemma_] = 0\n",
    "      freq_[lemma_] += f\n",
    "\n",
    "freq = Counter(freq_)\n",
    "freqAcc = sum([freq[w] for w in freq])\n",
    "\n",
    "medicalPrefixes.update(otherMedicalPrefixes)\n",
    "prefixes = list(set(medicalPrefixes.keys()))\n",
    "suffixes = list(set(medicalSuffixes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "labelResult = widgets.Label()\n",
    "labelBox = widgets.Label('Compound word')\n",
    "textBox = widgets.Text()\n",
    "button = widgets.Button(description='Submit')\n",
    "\n",
    "def submit_(b):\n",
    "    division_ = divideCompoundWord(textBox.value)\n",
    "    labelResult.value = ' '.join(division_)\n",
    "\n",
    "button.on_click(submit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d190586bd81b4ffca389e3d920cb84da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Compound word')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd164211d6b4cfe92db42feb4b8ab9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b977cd2992b1429bb4bf036fa79fba7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ea916ed79640e3a6af385a42b0d8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(labelBox)\n",
    "display(textBox)\n",
    "display(button)\n",
    "display(labelResult)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
