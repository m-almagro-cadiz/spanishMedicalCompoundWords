{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is a preliminary prototype for medical word decomposition\n",
    "# It is a heuristic method based on statistical information on word frequency\n",
    "import re, tables, pickle, itertools\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File with word frequencies within the CIE-10-ES\n",
    "pathCieSnFrequencies = 'data/cie_sn_frequencies.csv'\n",
    "# File with the part-of-speech tags for each word\n",
    "pathTags = 'data/tags.csv'\n",
    "# File with the lemmas for each word\n",
    "pathLemmas = 'data/extendedLemmas.csv'\n",
    "# File with the list of common medical prefixes\n",
    "pathPrefixes = 'data/medicalPrefixes.csv'\n",
    "# File with the list of rare medical prefixes\n",
    "pathOtherPrefixes = 'data/otherMedicalPrefixes.csv'\n",
    "# File with the list of common medical suffixes\n",
    "pathSuffixes = 'data/medicalSuffixes.csv'\n",
    "# File with the list of word embedding files in which each word is found\n",
    "pathWordIndex = 'data/wordIndex.obj'\n",
    "# Folder with the word embedding files\n",
    "pathConceptNetEmbeddings = 'embeddings/'\n",
    "\n",
    "# Read word frequencies\n",
    "with open(pathCieSnFrequencies, encoding='utf8') as f: freq = Counter({line.strip().split('\t')[0]:int(line.strip().split('\t')[1]) for line in re.split('\\n', f.read()) if line})\n",
    "\n",
    "# Read word POS tags\n",
    "with open(pathTags, 'r', encoding='utf8') as f: wordTag = {line.strip().split('\\t')[0]:{element.split(' ')[0]:element.split(' ')[1] for element in line.strip().split('\\t')[1::]} for line in re.split('\\n', f.read())}\n",
    "\n",
    "# Read lemmas\n",
    "with open(pathLemmas, 'r', encoding='utf8') as f: lemmaList = [line.strip().split('\\t') for line in re.split('\\n', f.read())]\n",
    "\n",
    "# Read common prefixes\n",
    "with open(pathPrefixes, encoding='utf8') as f: medicalPrefixes = {prefix.split('\\t')[0].lower().replace('-', '').strip():[e.strip() for e in prefix.split('\\t')[1::]] if len(prefix.split('\\t')) > 1 else '' for prefix in re.split('\\n', f.read()) if prefix}\n",
    "\n",
    "# Read rare prefixes\n",
    "with open(pathOtherPrefixes, encoding='utf8') as f: otherMedicalPrefixes = {prefix.split('\\t')[0]:[e.strip() for e in prefix.split('\\t')[1::]] if len(prefix.split('\\t')) > 1 else '' for prefix in re.split('\\n', f.read()) if prefix}\n",
    "\n",
    "# Read suffixes\n",
    "with open(pathSuffixes, encoding='utf8') as f: medicalSuffixes = {prefix.split('\\t')[0].lower().replace('-', '').strip():[e.strip() for e in prefix.split('\\t')[1::]] if len(prefix.split('\\t')) > 1 else '' for prefix in re.split('\\n', f.read()) if prefix}\n",
    "\n",
    "# Read word-file associations\n",
    "with open(pathWordIndex, 'rb') as f: wordIndex = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lemmas given a word\n",
    "def getLemmas(word):\n",
    "    if word in splittedLemmas:\n",
    "        return {word_ for word_ in splittedLemmas[word] if len(word_) > 3}\n",
    "    else:\n",
    "        return {word}\n",
    "\n",
    "# Get the absolute frequency given a word\n",
    "def getAbsFreq(word):\n",
    "   lemmas_ = list()\n",
    "   if word in splittedLemmas:\n",
    "      lemmas_ = getLemmas(word)\n",
    "   elif word not in freq:\n",
    "      word_ = unidecode(word).lower()\n",
    "      lemmas_ = getLemmas(word_)\n",
    "   f = 0\n",
    "   if lemmas_:\n",
    "      for lemma_ in lemmas_:\n",
    "         if lemma_ in freq:\n",
    "            f += freq[lemma_]\n",
    "   elif word in freq:\n",
    "      f = freq[word]\n",
    "   return f\n",
    "\n",
    "# Get the relative frequency given a word\n",
    "def getRelFreq(word):\n",
    "   return getAbsFreq(word) / freqAcc\n",
    "\n",
    "# Get the tags given a word\n",
    "def getTags(word):\n",
    "   tagsFromLemmas = set()\n",
    "   if word in splittedLemmas:\n",
    "      filteredTags_, tags_ = set(), set()\n",
    "      for tag_ in ['a', 'v', 'n', 'r']:\n",
    "         if (word, tag_) in lemmas:\n",
    "            tags_.update([lemma_[1] for lemma_ in lemmas[(word, tag_)]])\n",
    "            filteredTags_.update([lemma_[1] for lemma_ in lemmas[(word, tag_)] if len(lemma_[0]) > 3])\n",
    "      tagsFromLemmas = tags_ - filteredTags_\n",
    "   return tagger[word] - tagsFromLemmas if word in tagger else {}\n",
    "\n",
    "# Check if two words contain common tags\n",
    "def isSharedElement(tags1, tags2):\n",
    "   shared = False\n",
    "   for tag in tags1:\n",
    "      if tag in tags2:\n",
    "         shared = True\n",
    "         break\n",
    "   return shared\n",
    "\n",
    "# Get the possible separations given a word\n",
    "def getCandidateSubwords(word, prefixes, suffixes=[], reverse=False):\n",
    "   possibilities = list()\n",
    "   if len(word) > 3:\n",
    "      word_ = ''.join(reversed(word)) if reverse else word\n",
    "      i, subwords, subwords__ = 0, [[(list(), word_.lower())]], list()\n",
    "      while i < len(prefixes):\n",
    "         if len(subwords) == 1 or len(prefixes[i]) > 2:\n",
    "            for subword in subwords[-1]:\n",
    "               if subword[1].startswith(prefixes[i]):\n",
    "                  # Getting subword\n",
    "                  subword_ = re.sub(r'^\\W+', '', subword[1][len(prefixes[i])::]) # Deleting '-'\n",
    "                  if subword_ and prefixes[i][-1] in ['a', 'e', 'i', 'o', 'u']: # Deleting duplicated r\n",
    "                     subword_ = re.sub(r'^r(r)', r'\\1', subword_)\n",
    "                     if subword_ and len(prefixes[i]) > 1 and prefixes[i][-1] != subword_[0]: # Adding missing letter\n",
    "                        subwords__.append((subword[0] + [prefixes[i]], prefixes[i][-1] + subword_))\n",
    "                  startsWith_bp = prefixes[i][-1] != 'm' or (len(subword_) > 0 and subword_[0] in ['b', 'p'])\n",
    "                  notStartsWith_bp = prefixes[i][-1] != 'n' or (len(subword_) > 0 and subword_[0] not in ['b', 'p'])\n",
    "                  if (startsWith_bp and notStartsWith_bp) or reverse:\n",
    "                     subwords__.append((subword[0] + [prefixes[i]], subword_))\n",
    "         i += 1\n",
    "         if i == len(prefixes) and len(subwords__) > 0:\n",
    "            subwords.append(subwords__)\n",
    "            i, subwords__ = 0, list()\n",
    "      subwords = [([''.join(reversed(prefix)) for prefix in reversed(e[0])], ''.join(reversed(e[1]))) if reverse else (e[0], e[1]) for group in reversed(subwords[1::]) for e in group]\n",
    "      wtag, fWord = getTags(word), getRelFreq(word) # Getting whole word tag and frequency\n",
    "      for subword in subwords:\n",
    "         fSubword = getRelFreq(subword[1])\n",
    "         subwordLemmas_ = getLemmas(subword[1])\n",
    "         notShort_ = max([len(lemma_) for lemma_ in subwordLemmas_]) > 3 if subwordLemmas_ else len(subword[1]) > 3\n",
    "         inSuffixes_ = subword[1] in suffixes\n",
    "         if (fSubword > 0 and notShort_):\n",
    "            swtag = getTags(subword[1]) # Getting subword tag\n",
    "            matchingTag = isSharedElement(wtag, swtag) # Comparing tags\n",
    "            matching = len(wtag) == 0 or len(swtag) == 0 or matchingTag\n",
    "            isVerb = ('v' in wtag and len(wtag) == 1) or ('v' in swtag and len(swtag) == 1)\n",
    "            em = -1\n",
    "            if word in wordIndex and subword[1] in wordIndex:\n",
    "               nb1 = pd.read_hdf(pathConceptNetEmbeddings + str(wordIndex[word][1]) + '.h5')\n",
    "               nb2 = pd.read_hdf(pathConceptNetEmbeddings + str(wordIndex[subword[1]][1]) + '.h5')\n",
    "               em = cosine_similarity([nb1.loc[wordIndex[word][0]].values, nb2.loc[wordIndex[subword[1]][0]].values])[0][1]\n",
    "            elif subword[1] in wordIndex:\n",
    "               em = -2\n",
    "            if fSubword == 0:\n",
    "               rate_ = -1\n",
    "            else:\n",
    "               rate_ = fWord  / fSubword\n",
    "            possibilities.append((subword, fWord, fSubword, rate_, em, 1 if matching else 0, 1 if isVerb else 0, len(subword[1]), len(subword[0]), len(''.join(subword[0])), 1 if inSuffixes_ else 0))\n",
    "      possibilities = sorted(possibilities, key=lambda tup: (-tup[7], tup[8], tup[3], -tup[4]))\n",
    "   return possibilities\n",
    "\n",
    "# Check if the possible separation satisfies the imposed restrictions\n",
    "def isCandidate(candidate):\n",
    "   if (candidate[3] < 5) or (candidate[2] > 0.00001 and candidate[3] < 10):\n",
    "      return True\n",
    "   else:\n",
    "      return False\n",
    "\n",
    "# Select the best choices (decompositions) for the corresponding word\n",
    "def selectCandidate(candidates):\n",
    "   candidates_ = [candidate for candidate in candidates if isCandidate(candidate) and candidate[6] == 0]\n",
    "   if len(candidates_) > 0:\n",
    "      max_f_subword = max([candidate[2] for candidate in candidates_])\n",
    "      min_l_subword = min([candidate[7] for candidate in candidates_ if 5 * candidate[2] > max_f_subword])\n",
    "      min_n_prefix = min([candidate[8] for candidate in candidates_ if candidate[7] == min_l_subword])\n",
    "      min_l_prefix = min([candidate[9] for candidate in candidates_ if candidate[8] == min_n_prefix])\n",
    "      min_conditions = [1 if 5 * candidate[2] > max_f_subword and candidate[7] == min_l_subword and candidate[8] == min_n_prefix and candidate[9] == min_l_prefix else 0 for candidate in candidates_]\n",
    "      conditions = [1 if min_conditions[i] == 1 and candidates_[i][6] == 0 and candidates_[i][2] > 0.0000002 and (candidates_[i][4] >= 0.4 or candidates_[i][4] <= -1) else 0 for i in range(len(candidates_))]\n",
    "      candidates_ = [candidates_[i] for i in range(len(candidates_)) if conditions[i] == 1]\n",
    "   return candidates_\n",
    "\n",
    "# Generate all possible decompositions of a word according to all the suffixes and prefixes\n",
    "def divideCompoundWord(word):\n",
    "   candidates = getCandidateSubwords(word, prefixes, suffixes)\n",
    "   compositions = selectCandidate(candidates)\n",
    "   if len(compositions) > 0:\n",
    "      compositions_ = [medicalPrefixes[prefix] for prefix in compositions[0][0][0]] + [[compositions[0][0][1]]]\n",
    "      return [list(e) for e in list(itertools.product(*compositions_))]\n",
    "   else:\n",
    "      return [[word]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building lemmas dictionary\n",
    "lemmas = dict()\n",
    "for line in lemmaList:\n",
    "   key = line[0].split(' ')\n",
    "   if len(key) == 1:\n",
    "      key.append('')\n",
    "   key = tuple(key)\n",
    "   values, nToRem = line[1::], list()\n",
    "   for v in range(len(values)):\n",
    "      value = values[v].split(' ')\n",
    "      if len(value) == 1:\n",
    "         value.append('')\n",
    "      value = tuple(value)\n",
    "      values[v] = value\n",
    "   if key not in lemmas:\n",
    "      lemmas[key] = list()\n",
    "   lemmas[key] = values\n",
    "\n",
    "# Extending tags from lemmas\n",
    "tagger = {word:{tag[0] for tag in wordTag[word]} for word in wordTag}\n",
    "for lemma in lemmas:\n",
    "   if lemma[0] not in tagger:\n",
    "      tagger[lemma[0]] = set()\n",
    "   tagger[lemma[0]].add(lemma[1])\n",
    "\n",
    "# Collecting derivational words without tags\n",
    "splittedLemmas = dict()\n",
    "for lemma in lemmas:\n",
    "   if lemma[0]:\n",
    "      if lemma[0] not in splittedLemmas:\n",
    "         splittedLemmas[lemma[0]] = set()\n",
    "      splittedLemmas[lemma[0]].update([lemma_[0] for lemma_ in lemmas[lemma]])\n",
    "      word_ = unidecode(lemma[0]).lower()\n",
    "      if word_ not in lemmas:\n",
    "         if word_ not in splittedLemmas:\n",
    "            splittedLemmas[word_] = set()\n",
    "         splittedLemmas[word_].update([lemma_[0] for lemma_ in lemmas[lemma]])\n",
    "\n",
    "# Gathering word counts from CIE-10-ES and SNOMED-CT using lemmas\n",
    "freq_ = dict()\n",
    "for w in freq:\n",
    "   lemmas_ = [w]\n",
    "   if w in splittedLemmas:\n",
    "      lemmas_ = splittedLemmas[w]\n",
    "   f = freq[w]\n",
    "   for lemma_ in lemmas_:\n",
    "      if lemma_ not in freq_:\n",
    "         freq_[lemma_] = 0\n",
    "      freq_[lemma_] += f\n",
    "\n",
    "freq = Counter(freq_)\n",
    "freqAcc = sum([freq[w] for w in freq]) # Get the cumulative frequency\n",
    "\n",
    "# Put all prefixes together\n",
    "medicalPrefixes.update(otherMedicalPrefixes)\n",
    "prefixes = list(set(medicalPrefixes.keys()))\n",
    "suffixes = list(set(medicalSuffixes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an interactive interface\n",
    "from ipywidgets import widgets\n",
    "labelResult = widgets.Label()\n",
    "labelBox = widgets.Label('Compound word')\n",
    "textBox = widgets.Text()\n",
    "button = widgets.Button(description='Submit')\n",
    "\n",
    "def submit_(b):\n",
    "    labelResult.value = ''\n",
    "    division_ = divideCompoundWord(textBox.value)\n",
    "    labelResult.value = '  |  '.join([' '.join(e) for e in division_])\n",
    "\n",
    "button.on_click(submit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0d064b0a9a4ddf85d92ad3038b3e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Compound word')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acb8263279d46309f8d9f46689e1214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6028b9930434eac996591da4ef86b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e37640f4fb049949c3533800cb36e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the most feasible decomposition\n",
    "display(labelBox)\n",
    "display(textBox)\n",
    "display(button)\n",
    "display(labelResult)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
